{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart with Ray Compiled Graphs\n",
    "Let’s get started with a simple Compiled Graphs CG example!\n",
    "\n",
    "The basic CG workflow is:\n",
    "Define an asyclic graph of Ray actor tasks, to be executed lazily.\n",
    "“Compile” the graph into an CG, with its own optimized execution path. After this, no changes to the CG are allowed.\n",
    "Execute the CG and `ray.get()` the results, like a normal Ray task.\n",
    "\n",
    "Step (2) is the reason we can get better performance than Ray Core. It also lets us schedule GPU-GPU communication for you, and propagate errors without hanging.\n",
    "\n",
    "To demonstrate this, we will show how a graph of Ray actor tasks may be executed with classic Ray Core, and then show what happens with the new CG backend.\n",
    "\n",
    "First, install Ray:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (2.43.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (8.1.8)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (3.18.0)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (1.1.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (24.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (6.30.1)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (6.0.2)\n",
      "Requirement already satisfied: aiosignal in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (1.5.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from ray) (2.32.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from jsonschema->ray) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from jsonschema->ray) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from jsonschema->ray) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from jsonschema->ray) (0.23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from requests->ray) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from requests->ray) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from requests->ray) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from requests->ray) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello world example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default execution with Ray Core\n",
    "Let’s start by defining and creating an application with two normal Ray actors. The second actor will echo the response from the first actor. If you’re running this on a machine with GPUs, feel free to add the `num_gpus=1` resource requirement to the actor definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:07:25,251\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class EchoActor:\n",
    "  def echo(self, msg):\n",
    "    return msg\n",
    "\n",
    "a = EchoActor.remote()\n",
    "b = EchoActor.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute with Ray Core, you can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "msg_ref = a.echo.remote(\"hello\")\n",
    "msg_ref = b.echo.remote(msg_ref)\n",
    "print(ray.get(msg_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution for this might look something like this:\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image01.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 1: Default execution with Ray core</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "Ray actor tasks are executed with RPCs to the actor. Each time a task is executed, the function argument “hello” is serialized into the task request and the “hello” return value is also serialized into the task response. This results in 4 copies of the “hello” value.\n",
    "\n",
    "If the value is larger, it may be stored in Ray’s shared-memory object store. This results in fewer copies (3) than the above, because values can be passed directly through the object store instead of being copied through the driver.\n",
    "\n",
    "However, because of the dynamic API, the classic Ray Core backend does not necessarily know how objects will be used by the time that they are created. For example, when allocating `a`’s return value, the task on `b` may not be submitted yet. Thus, we need to introduce additional protocols to dynamically track and garbage-collect values, in addition to the task execution RPC. The additional arrows to the object store in the below diagram represent these protocols.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image02.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 2: Default execution with Ray core</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "Many of these arrows need to happen synchronously to guarantee correctness, which adds run-time overhead.\n",
    "\n",
    "The dynamic nature of classic Ray Core’s API also makes it difficult to support peer-to-peer methods for data transfer like RDMA or NVIDIA’s NCCL. For example, if `a`  wants to send a tensor to `b` with NCCL, we have to also allocate resources on `b` to complete the transfer. Doing so without creating deadlock is very difficult in a dynamic environment, when `b` may be busy executing something else.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution with the Ray Core DAG API\n",
    "\n",
    "Compiled graphs restrict the application to static control flow to get around the above limitations with classic Ray Core. The API builds off of the classic Ray Core’s [DAG API](https://docs.ray.io/en/latest/ray-core/ray-dag.html).\n",
    "\n",
    "To execute the same application with the DAG API in Ray Core, you first define a lazily executed DAG like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.dag\n",
    "import time\n",
    "\n",
    "# Define a DAG for lazy execution.\n",
    "with ray.dag.InputNode() as inp:\n",
    "  # Bind the actor task to an input placeholder.\n",
    "  intermediate_inp = a.echo.bind(inp)\n",
    "  dag = b.echo.bind(intermediate_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces the following DAG, where the input value is provided by the driver during `dag.execute`:\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image03.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 3: Vanilla Ray Core DAG execution (non-accelerated)</em></figcaption>\n",
    "</figure>\n",
    "Now we can execute the DAG with the classic Ray Core backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n",
      "took 0.0023351041600108147\n",
      "took 0.00244308914989233\n",
      "took 0.001971593126654625\n",
      "took 0.001755282748490572\n",
      "took 0.0017960895784199238\n",
      "took 0.0017983769066631794\n",
      "took 0.0020655961707234383\n",
      "took 0.0019308342598378658\n",
      "took 0.00178559310734272\n",
      "took 0.0017898539081215858\n"
     ]
    }
   ],
   "source": [
    "# Execute the DAG with different arguments:\n",
    "print(ray.get(dag.execute(\"hello\")))\n",
    "print(ray.get(dag.execute(\"world\")))\n",
    "\n",
    "# Time the execution:\n",
    "for _ in range(10):\n",
    "  start = time.perf_counter()\n",
    "  ray.get(dag.execute(\"hello\"))\n",
    "  print(\"took\", time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this executes in exactly the same way as the above, using the classic Ray Core backend. Thus, it will have the same potential overheads, etc. The only difference is in the API: we first define a DAG with a placeholder for the `“hello”` argument, and then we provide the value at execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *NEW* Execution with Ray Compiled Graphs\n",
    "Now let’s try executing this as with the CG backend instead. To do this, we add in a new call to compile the DAG. Now, execution may be much faster because we pre-allocate the execution resources at compile time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompiledDAG(05224b2e93904b8288834435f8488706)\n"
     ]
    }
   ],
   "source": [
    "# Compile the DAG.\n",
    "cdag = dag.experimental_compile()\n",
    "print(cdag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call does a couple things under the hood:\n",
    "* The backend statically allocated input and output buffers in the shared-memory object store for each actor task, instead of dynamically allocating them each time the DAG is executed. These buffers are reused at execution time, and actors always push results directly to the process that needs them. Each buffer is initialized with 100MB of capacity and can be resized if larger values are passed.\n",
    "* The backend also allocates the actor’s execution loop ahead of time. Instead of waiting for an RPC to execute its next task, each actor waits in a loop for the arguments (passed via the allocated buffers) for the next `echo` task.\n",
    "Note that the task execution happens on a background thread. Thus, the actor may still execute other tasks as normal, but these will now execute concurrently with the aDAG tasks.\n",
    "\n",
    "Compilation looks something like this: the driver coordinates with the actors and the object store to allocate the empty object buffers and begin an execution loop on each actor.\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image04.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 4: Result of compiling a Ray DAG</em></figcaption>\n",
    "</figure>\n",
    "Now, the per-DAG execution time should be much faster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.001427993644028902\n",
      "took 0.0005993461236357689\n",
      "took 0.0002647195942699909\n",
      "took 0.00021086819469928741\n",
      "took 0.00022509926930069923\n",
      "took 0.00021831924095749855\n",
      "took 0.00019855890423059464\n",
      "took 0.00022367527708411217\n",
      "took 0.0002134847454726696\n",
      "took 0.00021244678646326065\n"
     ]
    }
   ],
   "source": [
    "# Time the execution with aDAG\n",
    "for _ in range(10):\n",
    "  start = time.perf_counter()\n",
    "  ray.get(cdag.execute(\"hello\"))\n",
    "  print(\"took\", time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we no longer need to dynamically dispatch tasks and allocate objects. Instead, the execution flows directly from one process to the next. Note that now, the only arrows that the driver is involved in are the ones to provide the DAG input and read the DAG output, both of which can be done through shared memory.\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image05.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 5: Accelerated execution with Ray Compiled Graphs</em></figcaption>\n",
    "</figure>\n",
    "Once you’re done, you can tear down the compiled graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:07:37,014\tINFO compiled_dag_node.py:2109 -- Tearing down compiled DAG\n",
      "2025-03-20 16:07:37,016\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(EchoActor, 64110de81eb4c7d069b66b1c01000000)\n",
      "2025-03-20 16:07:37,016\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(EchoActor, f27572b741be117cbc55949b01000000)\n",
      "2025-03-20 16:07:37,022\tINFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit\n",
      "2025-03-20 16:07:37,022\tINFO compiled_dag_node.py:2143 -- Teardown complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Tear down the DAG\n",
    "cdag.teardown()\n",
    "\n",
    "print(ray.get(a.echo.remote(\"done\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which returns the cluster to a clean state:\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image06.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 6: Ray Cluster after CG pipeline cleanup</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "At this point, the CG’s resources are collected, and the background CG execution loop is stopped. You can now:\n",
    "* Execute classic Ray actor tasks on the actor, without needing to guard against races with the background CG thread\n",
    "* Define and compile a new CG using the same or other actors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing across multiple actors\n",
    "With this basic API, you can stitch together tasks across multiple actors in a variety of ways.\n",
    "For example, a common pattern you might see in GPU applications is SPMD, where all processes execute the same program over different data in lockstep. In [tensor-parallel inference](https://docs.vllm.ai/en/latest/serving/distributed_serving.html), for example, each actor might hold a different shard of a model, and we pass the same input to all actors.\n",
    "\n",
    "Here’s an example of how you might write such a program. Again, we start by creating normal Ray actors, in this case 3 of them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class EchoActor:\n",
    "  def echo(self, msg):\n",
    "    return msg\n",
    "\n",
    "N = 3\n",
    "actors = [EchoActor.remote() for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define and compile an aDAG that passes the same input placeholder to all actors. Here, we use the [MultiOutputNode](https://docs.ray.io/en/latest/ray-core/ray-dag.html#ray-dag-with-multiple-multioutputnode) syntax to wrap the outputs. This syntax is necessary when we have more than one output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DAG for lazy execution.\n",
    "with ray.dag.InputNode() as inp:\n",
    "  # Bind each actor task to the same input placeholder.\n",
    "  outputs = [actor.echo.bind(inp) for actor in actors]\n",
    "  dag = ray.dag.MultiOutputNode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a DAG like this:\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image07.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 7: Ray CG across multiple parallel Actors</em></figcaption>\n",
    "</figure>\n",
    "Which we can compile and execute like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hello', 'hello']\n"
     ]
    }
   ],
   "source": [
    "cdag = dag.experimental_compile()\n",
    "# Execute the DAG with different arguments:\n",
    "print(ray.get(cdag.execute(\"hello\")))\n",
    "# Expected: [\"hello\", \"hello\", \"hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, all actors that are on the same Ray node will share the same physical input buffer, which is synchronized by the Ray aDAG backend. This helps reduce the per-task overhead from serializing the task arguments, allocating memory for the arguments, and invoking the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining execution across different actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to pipeline the execution across different actor tasks? One example of this is (pipeline-parallel inference)[https://docs.vllm.ai/en/latest/serving/distributed_serving.html], where we pass intermediate outputs from one actor to the next through shared memory, and the data transfers should be pipelined with the compute tasks. We can pipeline execution across different actors by executing the same DAG multiple times before retrieving the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ray.dag.InputNode() as inp:\n",
    "  for actor in actors:\n",
    "    # Pass each actor task output as input to the next actor task.\n",
    "    inp = actor.echo.bind(inp)\n",
    "  dag = inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a DAG like this:\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image08.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 8: Ray CG over multiple pipelined Actors</em></figcaption>\n",
    "</figure>\n",
    "Which we can compile and execute like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:07:49,939\tINFO compiled_dag_node.py:2109 -- Tearing down compiled DAG\n",
      "2025-03-20 16:07:49,941\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(EchoActor, 728be378e1730abb2d37a78301000000)\n",
      "2025-03-20 16:07:49,941\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(EchoActor, 9856d387c8b630a9def5664701000000)\n",
      "2025-03-20 16:07:49,942\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(EchoActor, 98b52fc024098f0585d9d49201000000)\n",
      "2025-03-20 16:07:49,947\tINFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit\n",
      "2025-03-20 16:07:49,948\tINFO compiled_dag_node.py:2143 -- Teardown complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello0', 'hello1', 'hello2']\n"
     ]
    }
   ],
   "source": [
    "cdag = dag.experimental_compile()\n",
    "# Call dag.execute() several times. The executions will be pipelined across the different actors.\n",
    "refs = [cdag.execute(f\"hello{i}\") for i in range(N)]\n",
    "# Get the results, flushing the pipeline.\n",
    "print(ray.get(refs))\n",
    "# Expected: [\"hello0\", \"hello1\", \"hello2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to be aware of:\n",
    "* On the same actor, CG executions are ordered. I.e., if an actor has multiple tasks in the same CG, it will execute all of them to completion before executing on the next DAG input.\n",
    "* Across actors in the same CG, the execution may be pipelined. I.e., an actor may begin executing on the next DAG input while a downstream actor executes on the current one.\n",
    "\n",
    "For more examples of what kinds of CGs you can run, check out the general [Ray DAG API docs](https://docs.ray.io/en/latest/ray-core/ray-dag.html). Ray aDAG supports the same API except that:\n",
    "* You can only invoke actors that have already been created (e.g., `EchoActor.remote()`). I.e., you cannot use the `EchoActor.bind()` syntax with CGs.\n",
    "* Only actor tasks are supported. Non-actor tasks are not supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for `asyncio`\n",
    "If your CG driver is running in an asyncio event loop, use the async APIs to ensure that executing the CG and getting the results does not block the event loop. This requires a few changes for now. First, pass `enable_asyncio=True` to `dag.experimental_compile()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class EchoActor:\n",
    "  def echo(self, msg):\n",
    "    return msg\n",
    "\n",
    "actor = EchoActor.remote()\n",
    "with ray.dag.InputNode() as inp:\n",
    "  dag = actor.echo.bind(inp)\n",
    "\n",
    "cdag = dag.experimental_compile(enable_asyncio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `execute_async` to invoke the CG. Calling `await` on `execute_async` will return once the input has been submitted, and it returns a future that can be used to get the result. Then we get the result by calling `await` on the returned future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "ref = await cdag.execute_async(\"hello\")\n",
    "print(await ref)\n",
    "# Expected: hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing torch.Tensors between devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, you will also need to install Ray from [nightly wheels](https://docs.ray.io/en/latest/ray-overview/installation.html#daily-releases-nightlies) (or Ray 2.44 when it is released). In addition, you need the following:\n",
    "* [PyTorch with CUDA support](https://pytorch.org/get-started/locally/), \n",
    "* [CuPy](https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-pypi)\n",
    "* [NVIDIA’s NCCL library](https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html#down)\n",
    "* Other python packages, which you can install as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyarrow in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (19.0.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/gtc_code/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pyarrow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-GPU transfers\n",
    "With classic Ray Core, passing torch.Tensors between actors can easily become expensive, especially when transferring between devices. This is because Ray Core does not know the final destination device. Therefore, you may see unnecessary copies across devices other than the source and destination devices.\n",
    "\n",
    "Ray CG ship with native support for passing torch.Tensors between actors executing on different devices. Developers can now annotate the CG declaration to indicate the final destination device of a torch.Tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:11:42,293\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.dag\n",
    "\n",
    "import torch\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUActor:\n",
    "    def echo_device(self, tensor: torch.Tensor) -> str:\n",
    "        return str(tensor.device)\n",
    "\n",
    "actor = GPUActor.remote()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tensor will stay on the same origin's device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:11:45,026\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-03-20 16:11:45,027\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(GPUActor, cf0ce122f59366799197a5f201000000)\n",
      "2025-03-20 16:11:45,031\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-03-20 16:11:45,031\tINFO compiled_dag_node.py:2203 -- Teardown complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "with ray.dag.InputNode() as inp:\n",
    "  inp = inp.with_tensor_transport()\n",
    "  dag = actor.echo_device.bind(inp)\n",
    "\n",
    "cdag = dag.experimental_compile()\n",
    "print(ray.get(cdag.execute(torch.zeros(10))))\n",
    "# Expected: cpu\n",
    "cdag.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the destination device to be on GPU:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:11:47,464\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-03-20 16:11:47,465\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(GPUActor, cf0ce122f59366799197a5f201000000)\n",
      "2025-03-20 16:11:47,469\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-03-20 16:11:47,469\tINFO compiled_dag_node.py:2203 -- Teardown complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with ray.dag.InputNode() as inp:\n",
    "  inp = inp.with_tensor_transport(device=\"cuda\")\n",
    "  dag = actor.echo_device.bind(inp)\n",
    "\n",
    "cdag = dag.experimental_compile()\n",
    "print(ray.get(cdag.execute(torch.zeros(10))))\n",
    "# Expected: cuda:0\n",
    "cdag.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the Ray aDAG backend will copy the torch.Tensor to the GPU assigned to the GPUActor by Ray Core, like this:\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/image09.png\" style=\"background: white; padding: 10px;\">\n",
    "    <figcaption><em>Figure 9: CPU to GPU transfers</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "Of course, you can also do this yourself, but there are advantages to using CGs instead:\n",
    "* Ray CG can minimize the number of data copies made. For example, passing from one CPU to multiple GPU requires one copy to a shared memory buffer, and then one host-to-device copy per destination GPU.\n",
    "* In the future, this can be further optimized through techniques such as memory pinning, using zero-copy deserialization when the CPU is the destination, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU-GPU transfers via NCCL\n",
    "With classic Ray Core, GPU-GPU transfers can be done through the object store, but this typically requires many unnecessary copies through host memory. NVIDIA’s NCCL provides optimized GPU-GPU communication, avoiding these copies. The Ray CG developer preview comes with native support for p2p GPU transfers using NCCL. This is also specified using the `with_tensor_transport` hint.\n",
    "\n",
    "For example, to pass a tensor from one GPU actor to another, let’s first create sender and receiver actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.dag\n",
    "\n",
    "import torch\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUSender:\n",
    "  def send(self, shape):\n",
    "    return torch.zeros(shape, device=\"cuda\")\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class GPUReceiver:\n",
    "  def recv(self, tensor: torch.Tensor):\n",
    "    return tensor.shape\n",
    "\n",
    "sender = GPUSender.remote()\n",
    "receiver = GPUReceiver.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray CG allows us to specify that the transfer should be done via NCCL. This requires a synchronous operation between the two actors, but can improve overall performance significantly by avoiding unnecessary copies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:11:53,495\tINFO torch_tensor_nccl_channel.py:772 -- Creating NCCL group f4ffc6ac-bb86-43ed-b1e7-accb4877cb54 on actors: [Actor(GPUSender, 95fafa969f61278a3e9c370c01000000), Actor(GPUReceiver, 1d5a57fc52e27e4052543a6201000000)]\n",
      "2025-03-20 16:11:54,417\tINFO torch_tensor_nccl_channel.py:797 -- NCCL group initialized.\n",
      "2025-03-20 16:11:54,509\tINFO compiled_dag_node.py:2173 -- Tearing down compiled DAG\n",
      "2025-03-20 16:11:54,511\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(GPUSender, 95fafa969f61278a3e9c370c01000000)\n",
      "2025-03-20 16:11:54,511\tINFO compiled_dag_node.py:2178 -- Cancelling compiled worker on actor: Actor(GPUReceiver, 1d5a57fc52e27e4052543a6201000000)\n",
      "2025-03-20 16:11:55,037\tINFO compiled_dag_node.py:2200 -- Waiting for worker tasks to exit\n",
      "2025-03-20 16:11:55,038\tINFO compiled_dag_node.py:2203 -- Teardown complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(GPUSender pid=3868105)\u001b[0m Destructing NCCL group on actor: Actor(GPUSender, 95fafa969f61278a3e9c370c01000000)\n",
      "\u001b[36m(GPUReceiver pid=3868106)\u001b[0m Destructing NCCL group on actor: Actor(GPUReceiver, 1d5a57fc52e27e4052543a6201000000)\n"
     ]
    }
   ],
   "source": [
    "# Using NCCL for direct GPU-GPU transfer\n",
    "with ray.dag.InputNode() as inp:\n",
    "  dag = sender.send.bind(inp)\n",
    "  # Ray CG will automatically detect that the tensor should be transferred via NCCL\n",
    "  # but you can also explicitly use the `with_tensor_transport(transport=\"nccl\")` hint\n",
    "  dag = dag.with_tensor_transport()\n",
    "  dag = receiver.recv.bind(dag)\n",
    "\n",
    "# Creates a NCCL group across the participating actors.\n",
    "cdag = dag.experimental_compile()\n",
    "# Execute the DAG. Ray CG will orchestrate any NCCL ops.\n",
    "assert ray.get(cdag.execute((10, ))) == (10, )\n",
    "# Teardown the DAG. This also destroys the NCCL group.\n",
    "cdag.teardown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
